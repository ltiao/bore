{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression\n",
    "**COMP9418-17s2, W09 Tutorial**\n",
    "\n",
    "*School of Computer Science and Engineering, UNSW Sydney*\n",
    "\n",
    "- **Instructor**: Edwin V. Bonilla\n",
    "- **Teaching Assistant**: Louis Tiao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using Gaussian process (GP) models for regression. A Gaussian process defines a prior distribution over functions, which we use to obtain a posterior distribution over functions after having observed some data. As such, the predictions from a GP model take the form of a full predictive distribution.\n",
    "\n",
    "In this week's lab, we will focus gaining a practical understanding of these concepts - how to sample from a prior over functions, how to obtain and visualize the full predictive distribution, how the characteristics of a GP model changes as we vary some underlying hyperparameters, and so on. Finally, we will apply a GP regression model to a real-world dataset, and select a covariance function and its parameters to obtain automatic relevance determination (ARD), which can help us discard the irrelevant features. For a more complete treatment of Gaussian processes for regression, please refer to [GPML](http://www.gaussianprocess.org/gpml/) $\\S$2.2 (Rasmussen & Williams, 2006) or [MLaPP](https://www.cs.ubc.ca/~murphyk/MLbook/) $\\S$15.2 (Murphy, 2012)\n",
    "\n",
    "This lab makes extensive use of [GPFlow](https://github.com/GPflow/GPflow), with portions derived directly from their tutorial [GP Regression with GPflow](http://gpflow.readthedocs.io/en/latest/notebooks/regression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "The following Python packages are required for this exercise:\n",
    "\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `scikit-learn`\n",
    "- `tensorflow`\n",
    "- `GPflow`\n",
    "- `matplotlib`\n",
    "\n",
    "Most of these may be installed with `pip`:\n",
    "\n",
    "    pip install numpy pandas scikit-learn matplotlib tensorflow\n",
    "\n",
    "#### Tensorflow Installation\n",
    "\n",
    "The recommended way to install TensorFlow is in a `virtualenv` with `pip`. If your computer satisfies the [requirements](https://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support) to run TensorFlow with GPU support, you should definitely take advantage of it. You can install TensorFlow with GPU support\n",
    "\n",
    "    pip install tensorflow-gpu\n",
    "\n",
    "You can also take advantage of specialized CPU instruction sets for numerical computing by compiling TensorFlow from sources. This is outside the scope of this lab and we refer you to the [install guide](https://www.tensorflow.org/install/install_sources) for this.\n",
    "\n",
    "See the [full install guide](https://www.tensorflow.org/install/) for comprehensive instructions on installing TensorFlow for different environments with different options.\n",
    "\n",
    "#### GPflow Installation\n",
    "\n",
    "GPflow has not been released on PyPI. You can either download the package from the [official Github repo](https://github.com/GPflow/GPflow) and run `python setup.py install`, or install directly from Github via `pip` (recommended):\n",
    "\n",
    "    pip install git+https://github.com/GPflow/GPflow.git\n",
    "\n",
    "See the [install guide](http://gpflow.readthedocs.io/en/latest/intro.html#install) for further information.\n",
    "\n",
    "Here we import the required modules and set up our notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-notebook')\n",
    "np.set_printoptions(precision=3, \n",
    "                    edgeitems=5,  \n",
    "                    suppress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 12 # nbr. training points in synthetic dataset\n",
    "n_query = 100 # nbr. query points\n",
    "seed = 23 # set random seed for reproducibility\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's create a small synthetic dataset with `n_train` datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rng.rand(n_train, 1)\n",
    "Y = np.sin(12.*X) + .66*np.cos(25.*X) + rng.randn(n_train, 1)*.1 + 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.scatter(X, Y, marker='x', color='k')\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian process model is specified in terms of a mean function $m(\\mathbf{x})$ and kernel or covariance function $\\kappa(\\mathbf{x}, \\mathbf{x}')$. The Gaussian process prior on regression functions is denoted by\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) \\sim GP(m(\\mathbf{x}), \\kappa(\\mathbf{x}, \\mathbf{x}'))\n",
    "$$\n",
    "\n",
    "It is common to simply specify $m(x)=0$ since the GP is flexible enough to also model the mean arbitrarily well, so let's go ahead and specify a covariance function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels (covariance functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPFlow offers a number of commonly-used kernels, as well as an interface to define your own. See the tutorial [Using kernels in GPflow](http://gpflow.readthedocs.io/en/latest/notebooks/kernels.html) for more information. Let's take a look at the Radial Basis Function (RBF, also known as squared-exponential) kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF kernel with input_dim=2 and lengthscale=1.\n",
    "k = gpflow.kernels.RBF(1, lengthscales=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any finite set of $N$ points $\\mathbf{X}$, the GP prior above defines a joint Gaussian\n",
    "\n",
    "$$\n",
    "p(\\mathbf{f} \\mid \\mathbf{X}) = \\mathcal{N}(\\mathbf{f} \\mid \\boldsymbol{\\mu}, \\mathbf{K})\n",
    "$$\n",
    "\n",
    "where $K_{ij} = \\kappa(\\mathbf{x}_i, \\mathbf{x}_j)$ and $\\boldsymbol{\\mu} = (m(\\mathbf{x}_1, \\dotsc, \\mathbf{x}_N))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a set of `n_query` points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xq = np.linspace(-5., 5., n_query).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With one input fixed at 0, we can see how the kernel changes as the other input varies and moves away from 0. The method `compute_K` takes 2 arrays as arguments and gives the kernel computed pairwise between the points in the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4.5))\n",
    "\n",
    "ax.plot(Xq, k.compute_K(Xq, np.zeros(1).reshape(-1, 1)))\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$k(x, 0)$')\n",
    "ax.set_title('RBF Kernel ($\\ell={{{[0]:.2f}}}$)'\n",
    "             .format(k.lengthscales.value))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call `compute_K_symm` on $\\mathbf{X}$, which gives the covariance matrix $\\mathbf{K}$ described above. We can then sample functions $\\mathbf{f}$ from the joint Gaussian with covariance $\\mathbf{K}$. This is what it means to sample functions from a GP prior. Note that $\\mathbf{f}$ is a collection of function values evaluated at the finite set of points in $\\mathbf{X}$.  \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement the function `prior_samples`, which takes a 2d array of points, a GPFlow kernel and integer argument `n_samples`, and returns `n_samples` samples of function $\\mathbf{f}$ from the GP prior with zero mean function $m(\\mathbf{x})=0$ and the specified kernel.\n",
    "\n",
    "Note that while the set of `n_query` finite points `Xq` we're working with is 1-dimensional, we defined it to be a 2d array of shape `(n_query, 1)`. This is because GPFlow only accepts 2d arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE ###\n",
    "def prior_samples(x, kernel, n_samples=3):\n",
    "\n",
    "    # mean vector\n",
    "    mu = np.squeeze(np.zeros_like(x))\n",
    "    # covariance matrix\n",
    "    K = kernel.compute_K_symm(x)\n",
    "\n",
    "    # TODO: return n_samples from a joint Gaussian distribution\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION ###\n",
    "def prior_samples(x, kernel, n_samples=3):\n",
    "\n",
    "    mu = np.squeeze(np.zeros_like(x))\n",
    "    K = kernel.compute_K_symm(x)\n",
    "\n",
    "    return rng.multivariate_normal(mu, K, n_samples).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize draws from the GP prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.plot(Xq, prior_samples(Xq, k))\n",
    "\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$f(x)$')\n",
    "ax.set_title('Draws of $f(x)$ from GP prior')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effects of kernel parameters\n",
    "\n",
    "The kernel variance parameter $\\sigma_f$ (`k.variance`) controls the vertical scale of the function, while the length scale $\\ell$ (`k.lengthscales`) is the horizontal scale over which the function changes. The sampled functions look smoother as we increase the length scale. This makes sense as the effective distance between query points is inversely proportional to the length scale. Increasing the length scale causes the function values to become less correlated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(7, 7), \n",
    "                         subplot_kw=dict(xticks=[], yticks=[]))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i, ax_row in enumerate(axes):\n",
    "\n",
    "    variance = .05 * (i + 1)\n",
    "    \n",
    "    for j, ax in enumerate(ax_row):\n",
    "\n",
    "        len_scale = .1 + .7 * j\n",
    "\n",
    "        k = gpflow.kernels.RBF(1, variance=variance,\n",
    "                               lengthscales=len_scale)\n",
    "             \n",
    "        ax.plot(Xq, prior_samples(Xq, k))\n",
    "        ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "        if i == len(axes) - 1:\n",
    "            ax.set_xlabel('$\\ell={{{:.2f}}}$'.format(len_scale))\n",
    "           \n",
    "        if not j:\n",
    "            ax.set_ylabel('$\\sigma_f={{{:.2f}}}$'.format(variance))\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we observe the training set $(\\mathbf{X}, \\mathbf{y})$ we created earlier, where $y_i = f(\\mathbf{x}_i) + \\epsilon$ is the noisy observation of the function evaluated at $\\mathbf{x}_i$ and noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$. Given a query set $\\mathbf{X}_*$, we can obtain a full posterior distribution over the function outputs $\\mathbf{f}_*$. The joint distribution has the following form\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "  \\mathbf{y}\\\\\n",
    "  \\mathbf{f}_*\n",
    "\\end{pmatrix} \n",
    "\\sim\n",
    "\\mathcal{N} \\left (\n",
    "  0,\n",
    "  \\begin{pmatrix}\n",
    "    \\mathbf{K}_y   & \\mathbf{K}_* \\\\\n",
    "    \\mathbf{K}_*^T & \\mathbf{K}_{**}\n",
    "  \\end{pmatrix}\n",
    "\\right )\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\mathbf{K}_y    &= \\mathbf{K} + \\sigma_y^2\\mathbf{I} \\\\\n",
    "  \\mathbf{K}      &= \\kappa(\\mathbf{X}, \\mathbf{X}) \\\\\n",
    "  \\mathbf{K}_*    &= \\kappa(\\mathbf{X}, \\mathbf{X}_*) \\\\\n",
    "  \\mathbf{K}_{**} &= \\kappa(\\mathbf{X}_*, \\mathbf{X}_*)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then, using the standard method for conditioning Gaussians, the posterior predictive density is given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  p(\\mathbf{f}_* \\mid \\mathbf{X}_*, \\mathbf{X}, \\mathbf{y})\n",
    "  &= \\mathcal{N}(\\mathbf{f}_*, \\mu_*, \\Sigma_*) \\\\\n",
    "  \\mu_* &= \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{y} \\\\\n",
    "  \\Sigma_* &= \\mathbf{K}_{**} - \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{K}_*\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a GP regression model, conditioned on the synthetic training set (`X`, `Y`), with a squared-exponential kernel, and a sensible initial setting of the variance and length scale hyperparameters ($\\sigma_y, \\sigma_f, \\ell$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpflow.kernels.RBF(1, lengthscales=.2)\n",
    "m = gpflow.gpr.GPR(X, Y, kern=k)\n",
    "m.likelihood.variance = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given query set $\\mathbf{X}_*$, `m.predict_y` returns the means and variances $\\mu_*, \\Sigma_*$. Additionally, `m.predict_f_samples` yields samples from our posterior. We can use these to visualize the full posterior predictive distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "\n",
    "Complete the indicated portions of the function `plot_posterior_predictive` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE ###\n",
    "def plot_posterior_predictive(model, start=0., stop=1., n_query=100, \n",
    "                              n_samples=3, ax=None):\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    Xq = np.linspace(start, stop, n_query).reshape(-1, 1)\n",
    "    # COMPLETE ME #\n",
    "    # Compute the mean and variances of the predictive \n",
    "    # distribution, given the query points `Xq`\n",
    "    mean, var = ... \n",
    "\n",
    "    # scatter plot of the training points\n",
    "    ax.scatter(model.X.value, model.Y.value, marker='x', color='k')\n",
    "\n",
    "    # predictive mean mu_*\n",
    "    ax.plot(Xq, mean, 'b', lw=2., label='$m(x)$')\n",
    "\n",
    "    # COMPLETE ME #\n",
    "    # Plot a shaded region that represents mu_* +- 2 std(f_*)\n",
    "    ax.fill_between(np.squeeze(Xq),\n",
    "                    # mu_* - 2 std(f_*)\n",
    "                    # mu_* + 2 std(f_*)\n",
    "                    color='blue', alpha=.2)\n",
    "    \n",
    "    # samples from the posterior distribution\n",
    "    ax.plot(Xq, np.squeeze(model.predict_f_samples(Xq, n_samples)).T, \n",
    "            '--', alpha=.8)\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION ###\n",
    "def plot_posterior_predictive(model, start=0., stop=1., n_query=100, \n",
    "                              n_samples=3, ax=None):\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    Xq = np.linspace(start, stop, n_query).reshape(-1, 1)\n",
    "    mean, var = model.predict_y(Xq)\n",
    "\n",
    "    # scatter plot of the training points\n",
    "    ax.scatter(model.X.value, model.Y.value, marker='x', color='k')\n",
    "\n",
    "    # predictive mean mu_*\n",
    "    ax.plot(Xq, mean, 'b', lw=2., label='$m(x)$')\n",
    "\n",
    "    ax.fill_between(np.squeeze(Xq),\n",
    "                    np.squeeze(mean - 2*np.sqrt(var)),\n",
    "                    np.squeeze(mean + 2*np.sqrt(var)),\n",
    "                    color='blue', alpha=.2)\n",
    "\n",
    "    # samples from the posterior distribution\n",
    "    ax.plot(Xq, np.squeeze(model.predict_f_samples(Xq, n_samples)).T, \n",
    "            '--', alpha=.8)\n",
    "\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the posterior predictive distribution looks like under our GP model. It may seem that our model doesn't yield a great fit. The function is smooth and doesn't interpolate the observed values very well. Furthermore, there is a high degree of uncertainty at the observed points. If we believe our prior and our likelihood models then this is the right answer. However, as we shall see below, we can also estimate the model hyperparameters from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "plot_posterior_predictive(m, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we did with the prior distribution, we can visualize the effects of varying the length scale parameter $\\ell$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, figsize=(6, 8))\n",
    "#                          subplot_kw=dict(xticks=[], yticks=[]))\n",
    "fig.tight_layout()\n",
    "    \n",
    "for i, ax in enumerate(axes):\n",
    "\n",
    "    len_scale = .05 * (i + 1)\n",
    "\n",
    "    k = gpflow.kernels.RBF(1, lengthscales=len_scale)\n",
    "    m = gpflow.gpr.GPR(X, Y, kern=k)\n",
    "    m.likelihood.variance = .1\n",
    "\n",
    "    ax.set_title('$\\ell={{{:.2f}}}$'.format(len_scale))\n",
    "    plot_posterior_predictive(m, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the kernel parameters that result in the best fit, it is prohibitively slow to exhaustively search over a discrete grid of values to minimize some validation loss. Instead, we resort to an empirical Bayes approach (also known as type II maximum likelihood) where we maximize the *marginal likelihood*. This is much faster and amenable to gradient-based optimization methods.\n",
    "\n",
    "GPflow implements this in the method `m.optimize` and uses L-BFGS-B from SciPy as the underlying gradient-based optimization procedure. First, we initialize our GP model again an display its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpflow.kernels.RBF(1, lengthscales=.3)\n",
    "m = gpflow.gpr.GPR(X, Y, kern=k)\n",
    "m.likelihood.variance = 0.01\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us optimize these parameters and display the full predictive distribution again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.optimize()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this model fits the data much better than our initial model. It interpolates the observed values well and has the highest degree of uncertainty at points furthest away from the observed datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "plot_posterior_predictive(m, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other covariance functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned early, GPFlow implements a number of common covariance functions. Let us experiment with the following subset of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\n",
    "    gpflow.kernels.Matern12(1),\n",
    "    gpflow.kernels.Matern32(1),\n",
    "    gpflow.kernels.Matern52(1),\n",
    "    gpflow.kernels.RBF(1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "As we did for the squared-exponential (RBF) kernel, draw and visualize 3 samples from the GP priors specified by the kernels in the list defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE ###\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(6, 6), \n",
    "                         subplot_kw=dict(xticks=[], yticks=[]))\n",
    "fig.tight_layout()\n",
    "\n",
    "for ax, k in zip(axes.flat, kernels):\n",
    "    ### COMPLETE ME ###\n",
    "    ax.set_title(k.__class__.__name__)\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION ###\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(6, 6), \n",
    "                         subplot_kw=dict(xticks=[], yticks=[]))\n",
    "fig.tight_layout()\n",
    "\n",
    "for ax, k in zip(axes.flat, kernels):\n",
    "\n",
    "    ax.set_title(k.__class__.__name__)\n",
    "    ax.plot(Xq, prior_samples(Xq, k))\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "For each of the kernels in the list defined above, fit the hyperparameters to the training set and plot the full posterior predictive distribution as we did before for the squared-exponential kernel.\n",
    "\n",
    "Note that the log marginal likelihood is non-convex so a sensible initialization of the hyperparameters is crucial to avoid getting stuck in a local optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXERCISE ###\n",
    "fig, axes = plt.subplots(nrows=4, figsize=(6, 10))\n",
    "fig.tight_layout()\n",
    "    \n",
    "for ax, k in zip(axes, kernels):\n",
    "\n",
    "    m = gpflow.gpr.GPR(X, Y, kern=k)\n",
    "\n",
    "    ### COMPLETE ME ###\n",
    "    \n",
    "    ax.set_title(\n",
    "        ('{k} ($\\ell={{{l[0]:.2f}}},'\n",
    "         ' \\sigma_f={{{sf[0]:.2f}}},'\n",
    "         ' \\sigma_y={{{sy[0]:.2f}}}$)').format(k=k.__class__.__name__,\n",
    "                                               l=m.kern.lengthscales.value,\n",
    "                                               sf=m.kern.variance.value,\n",
    "                                               sy=m.likelihood.variance.value))\n",
    "\n",
    "    plot_posterior_predictive(m, ax=ax)\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION ###\n",
    "fig, axes = plt.subplots(nrows=4, figsize=(6, 10))\n",
    "fig.tight_layout()\n",
    "    \n",
    "for ax, k in zip(axes, kernels):\n",
    "\n",
    "    m = gpflow.gpr.GPR(X, Y, kern=k)\n",
    "    m.likelihood.variance = .01\n",
    "    m.kern.lengthscales = .1\n",
    "    m.optimize()\n",
    "    \n",
    "    ax.set_title(\n",
    "        ('{k} ($\\ell={{{l[0]:.2f}}},'\n",
    "         ' \\sigma_f={{{sf[0]:.2f}}},'\n",
    "         ' \\sigma_y={{{sy[0]:.2f}}}$)').format(k=k.__class__.__name__,\n",
    "                                               l=m.kern.lengthscales.value,\n",
    "                                               sf=m.kern.variance.value,\n",
    "                                               sy=m.likelihood.variance.value))\n",
    "\n",
    "    plot_posterior_predictive(m, ax=ax)\n",
    "            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world Dataset: Boston Housing Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we apply our GP regression model to the Boston housing prices dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 506 datapoints, with 13 continuous/categorical features and a single target, the median property value. You can view a full description of the dataset and its features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load this dataset into a Pandas DataFrame and view some of its summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "boston_df['MEDV'] = pd.Series(boston.target)\n",
    "boston_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can see how the median property value (`MEDV`) varies with the average number of rooms per dwelling (`RM`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "boston_df.plot.scatter(x='RM', y='MEDV', alpha=.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Relevance Determination (ARD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us fit a GP regression model to this dataset. We consider the squared-exponential covariance function as before, except now, we use the *anisotropic* variant of the kernel. That is, we consider a length scale *vector* of 13 positive values. \n",
    "\n",
    "These hyperparameter values determine how far you need to move along a particular axis in the input space for the function values to become uncorrelated. By estimating these values we effectively implement automatic relevance determination, as the inverse of the length scale determines the relevance of the dimension. If the length scale is very large, the covariance will practically become independence of that input, and effectively remove it from the inference (GPML $\\S$5.1 Rasmussen & Williams, 2006)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(normalize(boston.data), # scaling individual samples to have unit norm\n",
    "               np.atleast_2d(boston.target).T, # gpflow labels must be 2D \n",
    "               random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GPFlow, we can easily obtain an anisotropic kernel by specifying `ARD=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = gpflow.kernels.RBF(13, ARD=True)\n",
    "m = gpflow.gpr.GPR(X, y, k)\n",
    "m.likelihood.variance = .1\n",
    "m.optimize()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a Pandas Series to hold the length scales for ease of visualization and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(m.kern.lengthscales.value, index=boston.feature_names)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the bar chart of the length scales corresponding to each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "s.plot.bar(ax=ax)\n",
    "ax.set_ylabel('$\\ell_i$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the scatter plot with respect to the feature that has the smallest length scale, we find that it is indeed highly correlated with the median property value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "boston_df.plot.scatter(x=s.argmin(), y='MEDV', alpha=.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And vice versa for the feature with the largest length scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "boston_df.plot.scatter(x=s.argmax(), y='MEDV', alpha=.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "198px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "625px",
    "left": "0px",
    "right": "1067.67px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
